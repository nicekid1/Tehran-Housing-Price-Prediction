{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9477813,"sourceType":"datasetVersion","datasetId":5764603}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Project Objective\n\nThe primary goal of this project is to **develop a robust and accurate predictive model** that can estimate housing prices in **Tehran**, using structured tabular data collected from real estate listings.\n\nThis model aims to assist stakeholders such as **buyers, sellers, urban planners, and developers** in understanding price trends and making informed decisions based on property features.\n\nTo achieve this, I followed a well-defined, end-to-end data science workflow, which consists of the following stages:\n\n---\n\n### 1. Meet and Greet the Data  \n- Loaded the dataset and explored its structure, types, and distributions.\n- Performed initial quality checks and identified data types, formats, and completeness.\n\n### 2. Exploratory Data Analysis (EDA)  \n- Conducted statistical profiling and visualizations to uncover data patterns, trends, and relationships.\n- Analyzed key features like **Area, Room count, Facilities, Neighborhood**, and their correlation with price.\n- Investigated the effect of categorical variables such as **Parking, Elevator, and Warehouse** on pricing.\n- Detected and examined **outliers** and skewness in price and area distributions.\n\n### 3. Handling Missing Values  \n- Addressed missing values using appropriate imputation strategies:\n  - For example, imputing missing `Area` based on average area per room.\n  - Dropping rows with missing but non-critical values like `Address` (when limited in count).\n\n### 4. Feature Engineering  \n- Created new variables to capture complex relationships:\n  - `Facilities_sum`: total number of available facilities (Parking, Warehouse, Elevator).\n  - `Room_per_Area`: density of rooms relative to total area.\n  - `Neighborhood_house_count`: frequency of listings per neighborhood as a proxy for location popularity.\n- These engineered features enriched the dataset and significantly boosted model performance.\n\n### 5. Data Preprocessing  \n- Converted categorical variables into numerical form (e.g., label encoding for `Address`).\n- Applied log transformation to skewed variables like `Price` and `Area`.\n- Standardized selected features where needed, particularly for neural network training.\n\n### 6. Model Training and Evaluation  \n\n#### 6.1 Traditional Machine Learning Models  \n- Trained multiple regression models including:\n  - Linear Regression, Ridge, Lasso\n  - Decision Tree, Random Forest\n  - Gradient Boosting models: **XGBoost, LightGBM, CatBoost**\n  - K-Nearest Neighbors and MLP Regressor\n- Evaluated performance using **MAE, RMSE, and R²** metrics under different preprocessing scenarios.\n\n#### 6.2 Deep Learning Model  \n- Implemented a **fully connected ANN** using TensorFlow/Keras.\n- Designed with Batch Normalization and Early Stopping to stabilize and optimize training.\n- Compared its performance against tree-based ensembles to test effectiveness on structured data.\n\n---\n\n### 7. Summary and Final Insights  \n- Compared top-performing models based on predictive accuracy and generalization capability.\n- Found **XGBoost** and **Stacking Ensemble** models to be most effective, followed closely by the ANN model.\n- Highlighted the critical impact of feature engineering, log-transformation, and model selection on final performance.\n\n---\n\n### Final Goal\n\nThe ultimate aim of this project is to identify the most reliable and generalizable model for **housing price prediction in Tehran**, one that can be leveraged as a decision-support tool or serve as the basis for a production-grade real estate valuation system.\n\nThis work not only demonstrates core data science skills—**from data wrangling to advanced modeling**—but also bridges the gap between technical analysis and real-world applicability in urban economic contexts.","metadata":{"_uuid":"f137eda1-2138-422b-a66c-551dada1f7e9","_cell_guid":"006e262c-67cb-4880-8c48-3fa3c29d03eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"0 - Libs","metadata":{"_uuid":"2c8c69a0-2428-48ed-91ab-af44ca6ca24e","_cell_guid":"2406a455-abcc-4f56-b3d7-fcde9faa1824","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sys\nfrom copy import deepcopy\n%matplotlib inline\n\nfrom scipy.stats.mstats import winsorize\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import StackingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport optuna\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import keras","metadata":{"_uuid":"74679cfc-0e05-4143-afb9-1117ce782096","_cell_guid":"549117c9-7f4f-4a17-bc54-b98019786a23","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-06T07:09:06.242278Z","iopub.execute_input":"2025-07-06T07:09:06.242538Z","iopub.status.idle":"2025-07-06T07:09:28.712155Z","shell.execute_reply.started":"2025-07-06T07:09:06.242521Z","shell.execute_reply":"2025-07-06T07:09:28.711317Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Environment Setup & Library Imports\n\nTo begin the project, I imported a comprehensive set of libraries covering all essential stages of the machine learning pipeline. These include:\n\n- **Data manipulation and analysis**: `pandas`, `numpy`\n- **Data visualization**: `matplotlib`, `seaborn`\n- **Statistical techniques**: `scipy.stats` (for outlier handling via winsorization)\n- **Preprocessing and evaluation**: `scikit-learn` modules for:\n  - scaling\n  - encoding\n  - train-test splitting\n  - model training and evaluation (MAE, RMSE, R²)\n- **Machine Learning models**: \n  - Linear models: `LinearRegression`, `Ridge`, `Lasso`  \n  - Tree-based models: `DecisionTreeRegressor`, `RandomForestRegressor`  \n  - Instance-based models: `KNeighborsRegressor`  \n  - Ensemble models: `VotingRegressor`, `StackingRegressor`\n- **Boosting algorithms**: `XGBoost`, `LightGBM`, and `CatBoost`\n- **Neural Networks**: Built using `TensorFlow` and `Keras` (`Sequential`, `Dense`, `Adam`)\n- **Hyperparameter optimization**: `Optuna` was used to tune model parameters automatically.\n\nThis wide variety of tools and libraries provided the flexibility to experiment with various modeling techniques and find the most effective one for housing price prediction.","metadata":{"_uuid":"9ccdb3d2-2ead-45be-b849-adb2c5798663","_cell_guid":"cc6db072-1bc5-4ee5-ba26-d2e8ccea3671","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 1 - Meet and Greet the Data\n\nThe dataset used in this project is a structured CSV file containing detailed information about residential properties in **Tehran**. The data was loaded using `pandas` for further exploration and analysis:","metadata":{"_uuid":"cfeff4aa-4093-4d52-a399-4eba14705601","_cell_guid":"e31a0aee-4415-4981-b598-7018f875b344","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tehran-house-prices-dataset/TehranHouse.csv\")\n\ndf.head()","metadata":{"_uuid":"3a8c0e63-34a9-418e-b05b-95933fda446c","_cell_guid":"c7036045-cf63-471d-b04f-c6e396df04dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-06T07:09:38.373397Z","iopub.execute_input":"2025-07-06T07:09:38.373993Z","iopub.status.idle":"2025-07-06T07:09:38.418336Z","shell.execute_reply.started":"2025-07-06T07:09:38.373971Z","shell.execute_reply":"2025-07-06T07:09:38.417555Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Shape:',df.shape)","metadata":{"_uuid":"9a423ad6-d110-4d4c-9d7a-5e8996251ed6","_cell_guid":"6f130408-37ef-4d67-8292-a503acf0b111","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"_uuid":"e1972854-a505-4288-8670-c0eb56724339","_cell_guid":"cb05a474-2050-4cdf-a643-15c9acdbc6bf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Overview\n\nTo start the analysis, I examined the basic structure of the dataset using `df.info()`. This helped in understanding the number of records, data types, and the presence of any missing values.\n\nThe dataset contains **3,479 records** and **8 columns**, representing various characteristics of residential properties in Tehran. Here is a summary of the key observations:\n\n| Column        | Data Type | Non-Null Count | Description |\n|---------------|-----------|----------------|-------------|\n| `Area`        | object    | 3479           | Total area of the property (likely in square meters), stored as string (requires transformation). |\n| `Room`        | int64     | 3479           | Number of rooms. |\n| `Parking`     | bool      | 3479           | Indicates if the property has parking. |\n| `Warehouse`   | bool      | 3479           | Indicates if the property includes a storage room. |\n| `Elevator`    | bool      | 3479           | Whether the building has an elevator. |\n| `Address`     | object    | 3456           | Location/address of the property. Contains **23 missing values**. |\n| `Price`       | float64   | 3479           | Property price in local currency (Toman). |\n| `Price(USD)`  | float64   | 3479           | Converted price in USD for comparison. |\n\n### Observations:\n\n- No missing values in numerical or boolean columns.\n- The `Address` column contains **23 missing entries**, which will need to be handled in preprocessing.\n- The `Area` column is currently stored as a string (`object`), suggesting it may require cleaning and conversion to numerical format.\n- Target variables are present in both local currency and USD, providing flexibility for analysis and international comparison.\n\nThis initial inspection helped identify potential data quality issues and necessary transformations before proceeding to EDA and preprocessing steps.","metadata":{"_uuid":"7e26d2e8-563a-4746-a760-1c105f429321","_cell_guid":"55dfd2ce-bcf8-418d-81f8-d7bf83649235","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.describe()","metadata":{"_uuid":"529fba8c-c185-4b48-8361-8d56b3259994","_cell_guid":"378ea41f-0579-4c56-af1b-e01e3838192b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_count = df.isnull().sum()\nprint(missing_count)","metadata":{"_uuid":"ec53ec4e-6522-4bc8-be4d-8f4662df59d0","_cell_guid":"01e0990f-4ab4-4262-9cf3-34531b83d592","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2 - Exploratory Data Analysis (EDA) — Distribution of House Prices\n\nTo better understand the target variable, I analyzed the distribution of house prices, converted to **million Tomans** for better readability.","metadata":{"_uuid":"0004eb67-6f1a-4c8e-bd7e-166ea93052c7","_cell_guid":"500f500f-c635-4d7b-a225-4a940169a366","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Price_million_toman'] = df['Price'] / 1e7\n\nplt.figure(figsize=(10, 5))\nsns.histplot(df['Price_million_toman'], kde=True, bins=30, color='seagreen')\nplt.title('Distribution of House Prices (Million Toman)', fontsize=14)\nplt.xlabel('Price (Million Toman)')\nplt.ylabel('Count')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"9d70f507-5dd6-4a60-a0c5-49ad37193b7a","_cell_guid":"7bf3b148-4675-4793-a632-9e2c0539b828","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution of House Prices (Million Toman)\n\nThe dataset was divided into 30 bins to analyze the distribution of the price . The following observations can be drawn from the distribution:\n\n| Bin Range          | Count | Percentage (%) | Interpretation                          |\n|--------------------|-------|----------------|---------------------------------------|\n| 0.4 - 308.3        | 1810  | 52.37          | More than half of the properties fall into the smallest bin range, indicating a high concentration of lower values. |\n| 308.3 - 616.3      | 795   | 23.00          | The second largest bin still contains a significant portion (23%) of the data, showing gradual tapering. |\n| 616.3 - 924.3      | 341   | 9.87           | The frequency continues to decrease as values increase. |\n| 924.3 - 1,232.3    | 175   | 5.06           | Distribution is right-skewed with fewer high-value instances. |\n| 1,232.3 - 1,540.3  | 119   | 3.44           | Smaller number of properties in higher bins indicates rarer high values. |\n| 1,540.3 - 8,932.0  | 116   | ~3.4           | Very low counts across many upper bins, showing a long-tail distribution. |\n| > 8,932.0          | 2     | 0.06           | Extreme outliers or very rare very high values. |\n\n### 🔍 Key Insights:\n\n- **Highly right-skewed distribution**: More than 75% of the data is concentrated in the first two bins (up to ~616 units), showing that most properties are relatively small or have lower prices, depending on the feature.\n- **Long tail of rare high values**: The bins beyond ~1,500 units contain progressively fewer samples, indicating presence of rare but very large/expensive properties.\n- **Potential outliers**: The bins at the very high end with single-digit counts might represent outliers that may require special treatment in modeling.\n\nThis skewness and long-tail behavior must be carefully considered during modeling to avoid biasing the predictions towards the majority small-value properties and to appropriately handle extreme values.\n\n---","metadata":{"_uuid":"fb1dead8-ed7b-456a-ab76-be03348ed543","_cell_guid":"b7cc1213-a24e-48d2-aeb2-60586d05c72e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Area'] = pd.to_numeric(df['Area'],errors='coerce')","metadata":{"_uuid":"62a2be34-e5ac-44f7-ada8-9df83af008d3","_cell_guid":"0cf3530a-f5b7-48d2-91bb-3282d1903499","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_df = df[df['Area'] < 500]  \nplt.figure(figsize=(10, 5))\nsns.histplot(filtered_df['Area'], kde=True, bins=30, color='orchid')\nplt.title('Distribution of House Area (<500 m²)')\nplt.xlabel('Area (m²)')\nplt.ylabel('Count')\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"903eddb5-41d5-46b8-9b86-a1fa309835fc","_cell_guid":"fef30622-36fd-4426-8b8e-e64e70b0f9d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution Analysis of Property Area (m²)\n\nThe dataset’s property area values were divided into 30 bins to better understand how properties are distributed by size. Key insights from the distribution are:\n\n| Area Range (m²)    | Count | Percentage (%) | Interpretation                                   |\n|--------------------|-------|----------------|------------------------------------------------|\n| 30.0 - 44.5        | 100   | 2.91           | Smallest properties constitute a small fraction of the data. |\n| 44.5 - 88.0        | 1,519 | 44.23          | Nearly half of the properties are between ~45 and 88 m², indicating a common size range. |\n| 88.0 - 117.0       | 896   | 25.99          | A significant portion falls in mid-sized properties, reflecting typical family homes. |\n| 117.0 - 160.5      | 554   | 15.91          | Larger properties exist but with decreasing frequency. |\n| 160.5 - 189.5      | 120   | 3.49           | Relatively fewer large properties, indicating rarity of very spacious homes. |\n| > 189.5             | 141   | 4.06           | Very large properties (>189.5 m²) are scarce, suggesting a long-tail distribution. |\n\n### Key Observations:\n\n- The distribution of property sizes is **right-skewed**, with most properties clustered in the **small to medium range (44.5 to 117 m²)**.\n- This is typical in urban housing markets where smaller apartments and family homes dominate.\n- The presence of a **long tail** toward larger property sizes indicates some rare, high-area properties.\n- Very small (under 30 m²) and very large (above 400 m²) properties are rare.\n- This distribution highlights the importance of modeling techniques that can handle skewed features and potentially outliers in property area.\n\n---\n\nThis analysis of area distribution helps tailor feature engineering and model design decisions, such as considering transformations (e.g., log-scaling) or binning strategies to better capture size effects on price.","metadata":{"_uuid":"829c9387-9398-42ac-b1c5-61544de8e67f","_cell_guid":"ae0124e1-fd2f-4ad1-b18c-b3099a00e990","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nsns.countplot(x='Room', data=df, palette='viridis')\n\nplt.title('Number of Houses by Room Count')\nplt.xlabel('Number of Rooms')\nplt.ylabel('Count')\nplt.grid(axis='y')\nplt.show()","metadata":{"_uuid":"bc855e2c-4d34-4336-88e7-b8bdf60022fc","_cell_guid":"335f316c-9091-4c1e-a0cb-4ff91168cd91","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Distribution Analysis of Number of Rooms\n\nThe distribution of properties based on the number of rooms reveals important insights about housing types in the dataset:\n\n| Number of Rooms | Count | Percentage (%) | Interpretation                              |\n|----------------|-------|----------------|---------------------------------------------|\n| 0              | 10    | 0.29           | A very small number of properties with zero rooms, possibly data errors or special cases like studios or commercial spaces. |\n| 1              | 669   | 19.36          | Around one-fifth of the properties are single-room units, indicating a significant share of small apartments or studios. |\n| 2              | 1,943 | 56.22          | The majority (over 56%) have two rooms, making this the most common property size, likely reflecting typical small family homes. |\n| 3              | 731   | 21.15          | About 21% of the properties have three rooms, representing medium-sized family homes. |\n| 4              | 70    | 2.03           | Larger homes with four rooms are relatively rare. |\n| 5              | 33    | 0.95           | Very few properties have five rooms or more, indicating luxury or spacious homes are scarce. |\n\n### Key Takeaways:\n\n- The dataset is dominated by **small to medium-sized properties**, with 2-room units being the most prevalent.\n- Properties with zero rooms require further investigation to understand their nature or to correct possible data quality issues.\n- Larger properties with 4 or more rooms constitute a very small proportion of the market, consistent with urban housing constraints.\n- This distribution suggests the model should be sensitive to room count as a critical feature influencing price.\n\n---\n\nThis analysis informs feature importance and potential grouping strategies during modeling.","metadata":{"_uuid":"2378e6b2-e28d-4fad-935f-32cdbc916658","_cell_guid":"60d9e506-9e49-4bce-a14e-f78407432f89","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"cat_features = ['Parking', 'Warehouse', 'Elevator',]\n\nplt.figure(figsize=(12, 4))\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(1, 3, i+1)\n    sns.countplot(x=feature, data=df, palette='pastel')\n    plt.title(f'{feature} Distribution')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.xticks([0, 1], ['No', 'Yes'])\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"d8a56b4a-75c1-45db-9ea0-449365869e71","_cell_guid":"cfd67e27-6340-4011-8747-11638f5b1060","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Distribution Analysis of Binary Amenities\n\nThe dataset includes three key binary features representing property amenities: **Parking**, **Warehouse (Storage room)**, and **Elevator**. The distribution of these features is as follows:\n\n### Parking Availability\n| Parking | Count | Percentage (%) | Interpretation |\n|---------|-------|----------------|----------------|\n| No      | 527   | 15.25          | Approximately 15% of properties do not have parking facilities. |\n| Yes     | 2929  | 84.75          | The vast majority (over 84%) offer parking, highlighting its importance in Tehran's housing market. |\n\n### Warehouse (Storage Room) Availability\n| Warehouse | Count | Percentage (%) | Interpretation |\n|-----------|-------|----------------|----------------|\n| No        | 294   | 8.51           | Less than 10% lack a warehouse, suggesting most properties have additional storage space. |\n| Yes       | 3162  | 91.49          | Over 91% have warehouse space, an important amenity for urban residents. |\n\n### Elevator Availability\n| Elevator | Count | Percentage (%) | Interpretation |\n|----------|-------|----------------|----------------|\n| No       | 735   | 21.27          | About 21% of properties do not have elevators, possibly indicating low-rise buildings or older constructions. |\n| Yes      | 2721  | 78.73          | Nearly 79% are equipped with elevators, reflecting a preference or requirement for modern conveniences in apartment complexes. |\n\n### Summary Insights:\n\n- **High prevalence of parking and warehouse** features suggests these are standard expectations in most properties.\n- **Elevator availability is slightly less widespread**, possibly correlated with building height or age.\n- These binary amenities likely play a significant role in determining property prices and should be strongly considered in modeling.\n\n---\n\nIncluding these amenities as categorical features will help capture variations in property value linked to convenience and living standards.","metadata":{"_uuid":"48f82b4b-ae3f-4d0c-9d69-aa1964cbc4d9","_cell_guid":"86bfc2f9-ac05-4b65-8aad-7b91c50aa1fb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Price_million_toman'] = df['Price'] / 1e7  \nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='Area', y='Price_million_toman', hue='Room', palette='viridis')\n\nplt.title('Correlation between Area and Price (Million Toman)\\nColored by Number of Rooms', fontsize=14)\nplt.xlabel('Area (m²)')\nplt.ylabel('Price (Million Toman)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"a73fccfc-bc8e-41e8-b379-61342ff88a80","_cell_guid":"bac6b471-2d87-4b88-8a65-841759e516ac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Key Statistical Insights\n\n### Correlation between Area and Price\n- The Pearson correlation coefficient between **Area** (in square meters) and **Price** (in million Toman) is **0.723**.  \n- This indicates a strong positive linear relationship, meaning that generally, as the area increases, the price of the property also rises.  \n- This strong correlation justifies the importance of **Area** as a key predictive feature in the pricing model.\n\n---\n\n###  Average and Median Price by Number of Rooms\n\n| Number of Rooms | Count | Average Price (Million Toman) | Median Price (Million Toman) |\n|----------------|-------|-------------------------------|------------------------------|\n| 0              | 10    | 786.05                        | 35.25                        |\n| 1              | 669   | 170.75                        | 138                          |\n| 2              | 1943  | 333.03                        | 270                          |\n| 3              | 731   | 1093.87                       | 910                          |\n| 4              | 70    | 2560.03                       | 2150                         |\n| 5              | 33    | 3373.44                       | 2500                         |\n\n### Analysis:\n\n- **Price increases significantly with the number of rooms**, highlighting room count as a major factor affecting property value.  \n- The **average price** tends to be higher than the median price, especially noticeable in 0-room and 5-room properties, indicating **presence of outliers or skewed price distributions** in these groups.  \n- Properties with **zero rooms** have a surprisingly high average price but a low median, suggesting a few very expensive unusual cases skewing the mean. These may require further investigation or special handling.  \n- There is a steep price jump moving from 2-room to 3-room properties, and an even larger jump towards 4- and 5-room properties, reflecting the premium buyers pay for increased space and functionality.  \n\n---\n\nThese insights reinforce the importance of including both **Area** and **Number of Rooms** as critical features for the predictive model. Additionally, the skewness in price distributions suggests that robust modeling approaches that can handle outliers may be necessary.","metadata":{"_uuid":"7f4e2d69-ddfe-41a7-b5ea-9297c3ff1df2","_cell_guid":"865238ce-29db-44cf-96d0-96f4f978343d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"for feature in cat_features:\n    avg_price = df.groupby(feature)['Price'].mean() / 1e7\n    avg_price.plot(kind='bar', title=f'Average Price by {feature}', ylabel='Million Toman', color='mediumslateblue')\n    plt.xticks([0, 1], ['No', 'Yes'], rotation=0)\n    plt.grid(axis='y')\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"9ff03f4b-ff4b-45fe-813f-b8d2bc7d4236","_cell_guid":"c74bdc61-941e-4316-8c35-90996186b949","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Impact of Amenities on Average Property Price\n\nThe average property prices are summarized for three important binary amenities: Parking, Warehouse, and Elevator.\n\n| Amenity  | Availability | Count | Average Price (Million Toman) | Interpretation |\n|----------|--------------|-------|-------------------------------|----------------|\n| Parking  | No           | 527   | 172.77                        | Properties without parking have a significantly lower average price. |\n|          | Yes          | 2929  | 603.71                        | Presence of parking is associated with more than threefold increase in average price, highlighting its value. |\n| Warehouse| No           | 294   | 247.53                        | Properties lacking warehouse/storage space have lower average prices. |\n|          | Yes          | 3162  | 565.00                        | Availability of warehouse corresponds to a substantial price premium, reflecting its desirability. |\n| Elevator | No           | 735   | 364.05                        | Properties without elevator access tend to be cheaper on average. |\n|          | Yes          | 2721  | 584.98                        | Elevator availability is linked to higher average prices, indicating preference for this amenity. |\n\n### Key Takeaways:\n\n- **All three amenities show a positive association with property price**, suggesting they add considerable value.  \n- The **presence of parking** is the most impactful, with average prices more than tripling when parking is available.  \n- **Warehouse and elevator availability also correlate strongly with price**, reinforcing the idea that convenience and extra space are highly valued in Tehran’s housing market.  \n- These variables should be treated as important categorical features in the predictive modeling process.\n\n---\n\nIncorporating these amenities as binary features is expected to improve the model’s ability to accurately estimate housing prices by capturing differences in living standards and property convenience.","metadata":{"_uuid":"40c447da-e0f9-4507-948f-27d90a88fcb5","_cell_guid":"57bc57bb-ab82-42a0-8638-7c8923f068d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"top_areas = df.groupby(\"Address\")['Price_million_toman'].mean().sort_values(ascending=False).head(10)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=top_areas.values, y=top_areas.index, palette=\"rocket\")\n\nplt.title(\"Top 10 Most Expensive Neighborhoods\", fontsize=14)\nplt.xlabel(\"Average Price (Million Toman)\")\nplt.ylabel(\"Neighborhood\")\nplt.grid(axis='x', linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"e0c29a67-58be-4c10-85ed-587654e6b09f","_cell_guid":"7756209a-68e0-4a59-8cb2-68f4045ba1aa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Top 10 Most Expensive Neighborhoods in Tehran\n\nThe table below lists the top 10 neighborhoods with the highest average property prices in million Toman, along with the number of listings in each area.\n\n| Neighborhood | Number of Listings | Average Price (Million Toman) | Interpretation |\n|--------------|--------------------|-------------------------------|----------------|\n| Gandhi       | 1                  | 7000                          | The most expensive property, though only one listing, indicating a potential outlier or ultra-luxury estate. |\n| Lavasan      | 4                  | 4800                          | Exclusive and affluent suburb with very high average prices. |\n| Mahmoudieh   | 3                  | 3346.67                       | High-end residential area known for luxury properties. |\n| Vanak        | 2                  | 3270                          | Wealthy neighborhood with limited but expensive listings. |\n| Elahieh      | 17                 | 2678.64                       | Popular upscale district with a moderate number of listings. |\n| Argentina    | 2                  | 2516.5                        | Exclusive area with very high prices despite few listings. |\n| Zaferanieh   | 27                 | 2357.51                       | Well-established affluent neighborhood with significant market activity. |\n| Velenjak     | 22                 | 2138.14                       | Known for luxury apartments and villas. |\n| Farmanieh    | 57                 | 2091.73                       | Large number of listings, consistent with a high-end market. |\n| Niavaran     | 68                 | 1989.32                       | Northern district known for high living standards and property values. |\n\n### Insights:\n\n- The highest average prices belong to **neighborhoods traditionally known for luxury and exclusivity**.  \n- Some areas with very few listings (e.g., Gandhi, Argentina) may skew the average due to ultra-high priced properties or outliers.  \n- Neighborhoods like **Farmanieh** and **Niavaran** have a large number of listings with consistently high prices, indicating stable upscale markets.  \n- Location clearly plays a **critical role in pricing**, reinforcing the need to include geospatial or neighborhood features in the predictive model.\n\n---\n\nUnderstanding neighborhood price trends is essential for accurate property valuation, as location often outweighs other property attributes.","metadata":{"_uuid":"eca1b463-f350-48b0-8855-8169dcfc9ef4","_cell_guid":"027d3264-7e8a-4f7e-971c-74496a525f6a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"summary = []\n\ndf['Price_million'] = df['Price'] / 1e7\n\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n\nfor col in numeric_cols:\n    if col == 'Price' or col == 'Price_million':\n        continue\n    corr = df[[col, 'Price_million']].corr().iloc[0, 1]\n    summary.append({\n        'feature': col,\n        'type': 'numeric',\n        'correlation_with_price': round(corr, 3)\n    })\n\ncat_features = ['Parking', 'Warehouse', 'Elevator']\n\nfor col in cat_features:\n    mean_yes = df[df[col] == 1]['Price_million'].mean()\n    mean_no = df[df[col] == 0]['Price_million'].mean()\n    diff = mean_yes - mean_no\n    summary.append({\n        'feature': col,\n        'type': 'categorical',\n        'mean_price_difference': round(diff, 2)\n    })\n\nsummary_df = pd.DataFrame(summary)\n\nnumeric_summary = summary_df[summary_df['type'] == 'numeric'].sort_values(by='correlation_with_price', ascending=False)\ncat_summary = summary_df[summary_df['type'] == 'categorical'].sort_values(by='mean_price_difference', ascending=False)\n\nprint(\"Correlation with Price (Numeric Features):\")\ndisplay(numeric_summary)\n\nprint(\"\\nMean Price Difference (Categorical Features):\")\ndisplay(cat_summary)","metadata":{"_uuid":"b1cc8b76-e5e1-44b3-992e-0bcb2d4c037a","_cell_guid":"ad88e2d2-0a1e-4a99-907c-220a94ebf1a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation and Mean Price Difference Analysis\n\nThis section summarizes the relationship between various features and the target variable, **Price**, using correlation coefficients for numeric variables and mean price differences for categorical variables.\n\n### Numeric Features Correlation with Price\n\n| Feature           | Type    | Correlation with Price | Interpretation                          |\n|-------------------|---------|-----------------------|---------------------------------------|\n| Price_million_toman | Numeric | 1.000                 | Perfect correlation as it is the target variable itself. |\n| Price(USD)         | Numeric | 1.000                 | Perfect correlation with local currency price, confirming correct conversion. |\n| Area               | Numeric | 0.723                 | Strong positive correlation, indicating larger properties generally have higher prices. |\n| Room               | Numeric | 0.568                 | Moderate positive correlation, showing that more rooms tend to increase the price. |\n\n### Mean Price Difference for Categorical Features\n\n| Feature    | Type         | Mean Price Difference (Million Toman) | Interpretation                          |\n|------------|--------------|---------------------------------------|---------------------------------------|\n| Parking    | Categorical  | 428.73                                | Properties with parking are on average 428.73 million Toman more expensive. |\n| Warehouse  | Categorical  | 316.98                                | Availability of warehouse/storage increases average price by approximately 317 million Toman. |\n| Elevator   | Categorical  | 220.56                                | Presence of elevator adds on average 220.56 million Toman to property price. |\n\n### Key Takeaways:\n\n- **Area** and **Room count** are the most influential numeric features correlated with price, with area having a notably stronger effect.\n- The **presence of amenities** (parking, warehouse, elevator) significantly impacts prices, with parking having the largest average price premium.\n- These quantitative insights confirm the necessity to include both numeric and categorical features to build an effective predictive model.\n- Modeling efforts should pay attention to these variables during feature selection and engineering stages to capture their impact accurately.\n\n---\n\nThis comprehensive understanding guides subsequent modeling choices, ensuring the model reflects the true drivers of housing prices in Tehran.","metadata":{"_uuid":"e338d9b1-2ce7-409c-8958-f10273398bc8","_cell_guid":"fae4f5e2-d9e5-48ed-b9eb-b9714868c513","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 3. Handling Missing Values\n\nHandling missing data is a critical step in the data preprocessing pipeline, as incomplete records can negatively impact model performance and lead to biased or invalid results.\n\n### Missing Data Overview\n\n- The dataset contains missing values primarily in the **`Address`** column, with **23 missing entries** out of 3,479 records (~0.66% missing rate).\n- All other columns, including numeric and boolean features, have complete data.\n\n### Strategy for Handling Missing Values\n\nGiven the small proportion of missing data and the nature of the `Address` feature (categorical, location-based):\n\n- **Option 1: Imputation**  \n  For modeling purposes, imputing missing addresses with the most frequent neighborhood or a placeholder category such as `'Unknown'` can be effective to retain these samples.\n\n- **Option 2: Removal**  \n  Alternatively, due to the low percentage of missing values, these 23 records could be dropped without significant loss of information.\n\n- **Rationale**  \n  Since the `Address` field can influence property price significantly through location effects, it's important not to discard this feature entirely. Hence, imputing a meaningful placeholder allows the model to learn patterns for these cases.\n\n---\n\nBy carefully addressing missing values, we ensure data integrity and maintain the predictive power of location-based features in the housing price model.","metadata":{"_uuid":"b1000c46-7550-4d8c-aeba-8c7c4b82e3d7","_cell_guid":"161fe5cf-a5d5-4373-bfa6-d529a15cf3eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)","metadata":{"_uuid":"8a5b74f7-cf81-4188-ad51-88426c9be7ec","_cell_guid":"9f6a22dc-daa3-4958-b40b-946e6771ff5e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_area_by_room = df.groupby('Room')['Area'].mean()\n\ndef impute_area(row):\n    if pd.isna(row['Area']):\n        return mean_area_by_room[row['Room']]\n    else:\n        return row['Area']\n\ndf['Area'] = df.apply(impute_area, axis=1)","metadata":{"_uuid":"58a994cb-b68c-47a0-990a-b62592fc72ed","_cell_guid":"2c9c59f5-280a-4b04-ac30-bdb636ac32f0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputing Missing Area Values Based on Room Count\n\nIn addition to the missing `Address` values, the `Area` column may contain invalid or missing entries that can negatively affect the model. Rather than using a global average or median, we implemented a **group-based imputation strategy** for the `Area` feature.\n\n#### Methodology\n\n- First, we computed the **average area for each room count** using:\n\n```python\nmean_area_by_room = df.groupby('Room')['Area'].mean()","metadata":{"_uuid":"30d3521f-f292-4e60-8cf3-f000363119ca","_cell_guid":"5b54c2a2-f04c-4e5e-8ead-03c9b88ec314","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df = df.dropna(subset=['Address'])","metadata":{"_uuid":"f6cbfbb2-4f74-4b31-a814-01dbf77b8d22","_cell_guid":"288cfaf4-ca9c-4924-9ce4-56d6ef149e55","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dropping Missing Address Records\n\nTo handle missing values in the `Address` column, we removed rows where the value was missing:\n\n```python\ndf = df.dropna(subset=['Address'])","metadata":{"_uuid":"85469a6f-fe47-41b4-b618-6fcef048113d","_cell_guid":"c6b3f227-5894-4ac9-8f7e-b8b3263a67bc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)","metadata":{"_uuid":"dae895ad-9d84-4762-81a1-49868eafa617","_cell_guid":"cd84b18d-0c33-4d7a-a49e-d31b707775ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Missing Value Check\n\nAfter applying imputation and removal strategies, we confirmed that the dataset no longer contains any missing values","metadata":{"_uuid":"0dcc2533-173b-462e-aeb0-c38602db8fa8","_cell_guid":"f383c7af-2595-4322-a468-86541aec8a79","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4 - Feature Engineering\n\nFeature engineering plays a vital role in improving model performance by transforming raw data into informative features that better capture the underlying patterns affecting housing prices.","metadata":{"_uuid":"fd48b03f-6567-48b0-b4a0-d33978e96599","_cell_guid":"ed08250f-8ae9-4432-95ff-a225ec8befc4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"baseFeatures= ['Area', 'Room', 'Parking', 'Warehouse', 'Elevator']\nengineered_features =[]","metadata":{"_uuid":"79fcec19-8c6e-4ed7-9039-9369e05274f2","_cell_guid":"236e1ca3-ade1-425b-a32b-cb12c84fb859","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Categorization Strategy\n\nTo better understand the impact of feature engineering, we divided the dataset’s features into two distinct groups:\n\n#### 1. Base Features\nThese are the original features directly extracted from the dataset:","metadata":{"_uuid":"49bc814f-b9fa-46c5-9839-cd725965891f","_cell_guid":"4e18ae96-7085-46ad-84e9-fae4414e5acf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Facilities_sum'] = (\n    df['Parking'] +\n    df['Elevator'] +\n    df['Warehouse'] \n)\nengineered_features.append('Facilities_sum')","metadata":{"_uuid":"842c366b-f370-4a10-9304-16ef810b1419","_cell_guid":"b5f3b7f2-cd8a-4827-9f63-d0c1e306a5de","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Engineered Feature: `Facilities_sum`\n\nTo better capture the overall **level of amenities** available in each property, we engineered a new feature","metadata":{"_uuid":"eb14acc6-732a-46a5-bce6-ae896a1378bd","_cell_guid":"7f8d5b03-a0c0-4811-b357-8eba8272a189","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Room_per_Area'] = df['Room'] / df['Area']\nengineered_features.append('Room_per_Area')","metadata":{"_uuid":"8c13ce78-e2fb-4274-b545-697dafd2b08a","_cell_guid":"4142d54b-2b60-40d3-8195-ce335e1ce5ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Engineered Feature: `Room_per_Area`\n\nTo capture the spatial density of a property, we introduced the `Room_per_Area` feature","metadata":{"_uuid":"6a714502-6950-4716-bf4c-eea3288d0256","_cell_guid":"f3437077-bf8d-4ce4-ac23-406b58660ac8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df['Neighborhood_house_count'] = df.groupby('Address')['Address'].transform('count')\nengineered_features.append('Neighborhood_house_count')","metadata":{"_uuid":"04f75366-b8ff-4194-afed-15f8c17632f0","_cell_guid":"028df8ab-6818-4028-8b02-fd94b7ed89fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Engineered Feature: `Neighborhood_house_count`\n\nTo quantify the **popularity or density** of each neighborhood in the dataset, we introduced a new feature","metadata":{"_uuid":"d54a7f50-2b12-4555-8b53-5eae7017a9d8","_cell_guid":"eb1db8b3-355f-4fb1-9f58-9466f008f66e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.drop(['Price(USD)', 'Price_million_toman', 'Price_million'], axis=1, inplace=True)","metadata":{"_uuid":"35f67a34-6b1d-4e10-87a9-f4261529a450","_cell_guid":"5941605d-daa1-4d6a-86f9-cefc0bebd48f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dropping Redundant Price Columns\n\nAfter conducting exploratory data analysis and using price variations for visualization and interpretation, we decided to remove redundant pricing columns to simplify the dataset","metadata":{"_uuid":"7bb522cf-8946-497e-8fb6-9ed5fe72d6f8","_cell_guid":"8032c6eb-ee9b-4c47-898a-12d4a28aaf0a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"num_cols = df.select_dtypes(include=['int64', 'float64']).columns\noutlier_report = {}\n\nfor col in num_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    outliers = df[(df[col] < lower) | (df[col] > upper)]\n    outlier_report[col] = {\n        'total': df.shape[0],\n        'outliers': outliers.shape[0],\n        'percent': round(100 * outliers.shape[0] / df.shape[0], 2)\n    }\n\noutlier_df = pd.DataFrame(outlier_report).T.sort_values(by='percent', ascending=False)\ndisplay(outlier_df)","metadata":{"_uuid":"35ff5ecf-43a2-46ec-ac1c-0444e61cb72e","_cell_guid":"edd8d48a-4556-49e9-8ea8-c186481c8052","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outlier Detection Summary\n\nBefore proceeding to model training, we performed an outlier analysis on key numerical features to identify potentially extreme or anomalous values that could skew the model.\n\n| Feature                  | Total Records | Outliers Detected | Outlier Percentage |\n|--------------------------|----------------|--------------------|---------------------|\n| Room                    | 3456           | 1513               | 43.78%              |\n| Price                   | 3456           | 310                | 8.97%               |\n| Area                    | 3456           | 238                | 6.89%               |\n| Room_per_Area           | 3456           | 73                 | 2.11%               |\n| Neighborhood_house_count| 3456           | 0                  | 0.00%               |\n\n---\n\n#### Observations:\n\n- **Room**:\n  - Has a high percentage of outliers (~44%).\n  - This may reflect properties with **extremely high or unusually low room counts** that don’t match typical area distributions.\n  - May require capping or binning based on domain rules (e.g., typical homes rarely have more than 5–6 rooms).\n\n- **Price & Area**:\n  - Both contain noticeable but moderate levels of outliers (~7–9%).\n  - These may represent **luxury properties or rare deals** in the market.\n  - Consider winsorizing, log-transforming, or modeling with robust algorithms.\n\n- **Room_per_Area**:\n  - Relatively low outlier count (2.1%), but still important due to being a ratio metric.\n  - High values may represent **densely packed or low-quality units**.\n\n- **Neighborhood_house_count**:\n  - No outliers detected — consistent with expectations, as this is a derived aggregate from the dataset itself.\n\n---\n\n####Why Outliers Matter:\n\n- They can **distort model coefficients** in linear models.\n- In distance-based algorithms (like KNN), they can **inflate distances**.\n- In tree-based models, they can lead to **overfitting branches** for rare cases.\n- Some models (e.g., neural nets) assume **normalized inputs**, where outliers can dominate gradients.\n\n---\n\n###  Next Step:\n\nDepending on the modeling strategy, we may:\n- **Remove**, **cap**, or **transform** outliers.\n- Use robust models (e.g., tree-based algorithms).\n- Apply **scaling** or **log transformations** to reduce outlier impact.\n\nThis diagnostic step helps ensure the data is well-behaved and minimizes model bias caused by extreme cases.","metadata":{"_uuid":"7f7e02fa-7619-4076-9bcb-5e0f947dcd23","_cell_guid":"52bd45ca-255b-4913-8b75-a980a0b26b81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.info()","metadata":{"_uuid":"d33efb5c-1d60-4841-8558-9a79ea8da066","_cell_guid":"e133baf6-467a-4bce-9b40-81981514755c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5 - Data Preprocessing\n\nPreprocessing prepares the raw and engineered features for effective model training by ensuring consistent scale, appropriate encoding, and clean separation of training and testing data.","metadata":{"_uuid":"47ebef18-7b2e-4ffb-8ef6-ca8a918aeab2","_cell_guid":"f4433283-4ad7-423f-b6cf-02bb7283ede2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df[['Parking', 'Warehouse', 'Elevator']] = df[['Parking', 'Warehouse', 'Elevator']].astype(int)","metadata":{"_uuid":"4a5a4c06-2ed1-42ff-90d0-d8b9d5a72778","_cell_guid":"b60617f3-bc61-4f68-9d6d-afdd4f422507","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoding Binary Features\n\nThe binary features `Parking`, `Warehouse`, and `Elevator` were converted from boolean (`True`/`False`) to integer (`1`/`0`) to ensure compatibility with machine learning algorithms","metadata":{"_uuid":"d09d283f-db32-4e06-8c61-358a9be7c603","_cell_guid":"2de521a8-5cdc-4842-b018-254bff83caa7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"le = LabelEncoder()\ndf['AddressEncode'] = le.fit_transform(df['Address'])\ndf.drop(['Address'],axis=1,inplace=True)\nbaseFeatures.append('AddressEncode')","metadata":{"_uuid":"d4edbf2d-3242-40de-b8f6-76151c58fbc4","_cell_guid":"4d4b3829-64ec-40fb-9512-bf01bc2e7666","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Label Encoding for `Address`\n\nSince machine learning models require numerical input, the `Address` (neighborhood) column was transformed using **Label Encoding**","metadata":{"_uuid":"63730d19-fa27-4c6b-859a-897659ab8db3","_cell_guid":"a46b2b11-59bb-4691-ab8c-f3902a6f3c2b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df.select_dtypes(include=['number']).agg(['min', 'max'])","metadata":{"_uuid":"542c4628-c273-46e6-ac6e-2b8c4d6ba875","_cell_guid":"ab20e083-2cd6-42f4-a47a-2c9bacf29041","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Range and Scaling Considerations\n\nBelow is a summary of the main features’ value ranges after feature engineering and encoding:\n\n| Feature                  | Min     | Max            |\n|--------------------------|---------|----------------|\n| Area                     | 30.0    | 929.0          |\n| Room                     | 0       | 5              |\n| Parking                  | 0       | 1              |\n| Warehouse                | 0       | 1              |\n| Elevator                 | 0       | 1              |\n| Price                    | 3.6M    | 92.4B          |\n| Room_per_Area            | 0.0     | 0.086          |\n| Neighborhood_house_count | 1       | 161            |\n| AddressEncode            | 0       | 191            |\n\n---\n\n#### Key Insights:\n\n- The features exhibit **different value scales and ranges**:\n  - For example, `Area` ranges from 30 to 929, while `Room` is between 0 and 5.\n  - `Price` spans a very wide range from millions to tens of billions (Iranian Rial).\n  - Categorical or engineered features like `Neighborhood_house_count` and `AddressEncode` have wider ranges due to encoding.\n\n- **Scaling was not applied intentionally** because:\n  - The primary focus is on **boosting algorithms** (like XGBoost, LightGBM, CatBoost), which are generally **scale-invariant**.\n  - These models are robust to features with different scales and do not require normalization or standardization.\n  - Avoiding scaling helps keep feature interpretability intact and simplifies the pipeline.\n\n---\n\n#### When Scaling Might Be Needed:\n\n- If other models are used (e.g., Linear Regression, Neural Networks, KNN), scaling or normalization is often necessary.\n- For models sensitive to feature magnitude, such as distance-based or gradient-based methods, standard scaling or Min-Max scaling improves convergence and performance.\n\n---\n\n### Conclusion:\n\n- Given the choice of models focused on tree-based boosting algorithms, **skipping scaling is justified** and aligned with best practices.\n- The feature ranges are retained in their natural units, preserving interpretability and model robustness.","metadata":{"_uuid":"87fb5dc7-a67f-4895-8f66-3a190dad67c7","_cell_guid":"267b7095-8f7c-4a02-94d5-eeeef30d67c1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 6 - Train and Evaluate Models\n\nIn this phase, we trained and evaluated a variety of models to predict Tehran housing prices. The modeling process was divided into two main categories:\n\n### 6.1 Machine Learning Models\n\nWe implemented and compared several traditional machine learning regression algorithms, including:\n\n- **Linear Regression**  \n- **Ridge Regression**  \n- **Lasso Regression**  \n- **Decision Tree Regressor**  \n- **Random Forest Regressor**  \n- **K-Nearest Neighbors Regressor (KNN)**  \n- **Gradient Boosting Machines (XGBoost, LightGBM, CatBoost)**  \n- **Ensemble Methods (Voting Regressor, Stacking Regressor)**  \n\n#### Approach:\n\n- Hyperparameters were tuned using **GridSearchCV** or specialized libraries like **Optuna** to optimize performance.\n- Models were trained on the training set and evaluated on the hold-out test set.\n- Performance metrics included **Mean Absolute Error (MAE)**, **Root Mean Squared Error (RMSE)**, and **R² score**.\n\n### 6.2 Deep Learning Models\n\nWe also developed deep neural networks using TensorFlow/Keras, experimenting with:\n\n- Multi-layer Perceptrons (MLPs) with several dense layers.\n- Different architectures and hyperparameters such as:\n  - Number of layers and neurons\n  - Activation functions (ReLU, etc.)\n  - Optimizers (Adam)\n  - Learning rate and batch size\n  - Dropout for regularization\n\n#### Approach:\n\n- The network was trained to minimize the **Mean Squared Error (MSE)** loss.\n- Early stopping and model checkpoints were applied to avoid overfitting.\n- Final evaluation used the same metrics as traditional models for consistency.\n\n---\n\n### Summary:\n\nBy comparing classical machine learning models and deep learning architectures, we aimed to identify the best approach for housing price prediction in Tehran. The ensemble boosting methods generally showed strong performance due to their robustness and ability to capture complex nonlinear relationships. Deep learning models offered flexibility but required careful tuning and more computational resources.\n\n---","metadata":{"_uuid":"12820a93-228d-49e7-903c-1f10d899bc1e","_cell_guid":"ff82193a-67ca-4c57-b579-089488b73318","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"6-1- ML Modles","metadata":{"_uuid":"ad6421f8-ee62-4e46-9d51-8accc3ff3a71","_cell_guid":"c59c3b25-c2fc-4b29-9822-2ce987e0ae81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"models = {\n    'LinearRegression': LinearRegression(),\n    'Ridge': Ridge(),\n    'Lasso': Lasso(),\n    'DecisionTree': DecisionTreeRegressor(),\n    'RandomForest': RandomForestRegressor(),\n    'KNN': KNeighborsRegressor(),\n    'MLP': MLPRegressor(max_iter=1000),\n    'XGBoost': xgb.XGBRegressor(),\n    'LightGBM': lgb.LGBMRegressor(),\n    'CatBoost': cb.CatBoostRegressor(verbose=0) \n}","metadata":{"_uuid":"af1fc2cc-0322-4289-b4f7-d6a4866f74b2","_cell_guid":"f9821642-fad2-4433-85ba-6d4bf56d7d8f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_models(df, features, log_price=False, scale=False, log_area=False):\n    temp_df = df.copy()\n    \n    if log_price:\n        temp_df['Target'] = np.log1p(temp_df['Price'])\n    else:\n        temp_df['Target'] = temp_df['Price']\n\n    if log_area and 'Area' in features:\n        temp_df['Area'] = np.log1p(temp_df['Area'])\n\n    X = temp_df[features].copy()  \n\n    if scale:\n        scale_features = ['Area', 'Room_per_Area', 'Neighborhood_house_count', 'AddressEncode']\n        scaler = StandardScaler()\n        to_scale = [f for f in scale_features if f in X.columns]\n        X[to_scale] = scaler.fit_transform(X[to_scale])\n\n    y = temp_df['Target']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    results = []\n\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n\n        if log_price:\n            y_test_eval = np.expm1(y_test)\n            y_pred_eval = np.expm1(y_pred)\n        else:\n            y_test_eval = y_test\n            y_pred_eval = y_pred\n\n        mae = mean_absolute_error(y_test_eval, y_pred_eval)\n        rmse = np.sqrt(mean_squared_error(y_test_eval, y_pred_eval))\n        r2 = r2_score(y_test_eval, y_pred_eval)\n\n        results.append({\n            'Model': name,\n            'MAE': round(mae),\n            'RMSE': round(rmse),\n            'R2': round(r2, 3),\n            'Scaled': scale,\n            'LogPrice': log_price,\n            'LogArea': log_area,\n            'Features': features\n        })\n\n    return pd.DataFrame(results)","metadata":{"_uuid":"304a1f82-8cc9-4b27-b149-ca785d91deb2","_cell_guid":"75386e66-1cbf-49f8-909f-e18d4f0c1eeb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this phase, multiple regression models were trained and evaluated to predict housing prices in Tehran. The modeling approach was designed to be flexible, allowing experimentation with different preprocessing options such as log-transforming the target variable (price) and key features (e.g., area), as well as feature scaling.\n\n#### Key points of the approach:\n\n- **Model diversity:** A wide range of models was tested, including traditional linear models (Linear Regression, Ridge, Lasso), tree-based models (Decision Tree, Random Forest, LightGBM, CatBoost), K-Nearest Neighbors, and a Multi-Layer Perceptron (MLP) neural network.\n  \n- **Log-transformations:** To handle skewness and heteroscedasticity in price data, the target variable was optionally transformed using logarithms. Similarly, the area feature could be log-transformed to reduce skew and improve model learning.\n\n- **Feature scaling:** Selected numerical features were optionally standardized to zero mean and unit variance, to support models sensitive to feature scales.\n\n- **Evaluation metrics:** Models were evaluated on the hold-out test set using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R²). For log-transformed prices, predictions were back-transformed to the original scale for meaningful error interpretation.\n\n- **Comparison and selection:** Results across models and preprocessing setups were compiled to identify the best-performing algorithms and preprocessing combinations.\n\n#### Summary\n\nThis structured and flexible training pipeline enabled thorough benchmarking of diverse models under multiple data transformation scenarios. It facilitated insights into the impact of preprocessing choices on model accuracy, ultimately guiding selection of the most effective modeling approach for housing price prediction.","metadata":{"_uuid":"3a74e1f4-b610-47f9-95c3-d398d39adc05","_cell_guid":"b794ff99-98ab-4f93-9e7a-20516692580c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"scenarios = []\n\nfor log_price in [False, True]:\n    for scale in [False, True]:\n        for log_area in [False, True]:\n            scenarios.append((baseFeatures, log_price, scale, log_area))\n            scenarios.append((baseFeatures + engineered_features, log_price, scale, log_area))\n\nall_results = pd.concat([\n    evaluate_models(df, *scenario)\n    for scenario in scenarios\n], ignore_index=True)","metadata":{"_uuid":"25e2430c-bc8c-4901-ba7c-0f7b8da92a14","_cell_guid":"a2e909fb-299d-459c-ba12-25db225ffb85","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation Across Multiple Preprocessing Scenarios\n\nTo comprehensively understand the impact of data preprocessing on model performance, we designed a set of experiments covering different scenarios by varying:\n\n- Whether the target variable (`Price`) was log-transformed to reduce skewness.\n- Whether selected numerical features were standardized (scaled).\n- Whether the `Area` feature was log-transformed.\n- Whether only the base features or the base features combined with engineered features were used.\n\nThis resulted in a total of 16 distinct scenarios (2 options for each of the 3 preprocessing steps × 2 feature sets).\n\nFor each scenario, all defined models were trained and evaluated on the dataset. The results were aggregated into a single comprehensive table to facilitate direct comparison.\n\n#### Purpose and Benefits:\n\n- **Systematic exploration** of how transformations and feature engineering affect model accuracy.\n- Enables **robust selection** of the best combination of preprocessing and model type.\n- Provides insights into the **relative importance of engineered features** compared to base features alone.\n- Helps identify whether scaling or log-transformations improve performance, especially for models sensitive to feature distributions.\n\n---\n\nThis experimental design ensures that model evaluation is thorough and grounded in evidence, leading to more informed and reliable modeling decisions.","metadata":{"_uuid":"fefc0ecb-b035-4d96-909a-00e1028b7462","_cell_guid":"fe725e7e-5357-41e5-84c0-96ff2a29d61a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"display_columns = ['Model', 'MAE', 'RMSE', 'R2', 'Scaled', 'LogPrice', 'LogArea','Features']\nsorted_results = all_results.sort_values(by='RMSE', ascending=True)[display_columns]\nsorted_results.to_csv(\"models_by_RMSE.csv\", index=False)\nstyled_table = sorted_results.style \\\n    .highlight_min(subset=['MAE', 'RMSE'], color='green') \\\n    .highlight_max(subset=['R2'], color='green') \\\n    .format({\n        'MAE': '{:,.0f}',\n        'RMSE': '{:,.0f}',\n        'R2': '{:.3f}'\n    }) \\\n    .set_table_styles([\n        {'selector': 'th', 'props': [('font-size', '14px'), ('background-color', '#3a44d6')]},\n        {'selector': 'td', 'props': [('font-size', '13px')]}\n    ]) \\\n    .set_caption(\"Model Performance Comparison for Housing Price Prediction\")\n\nstyled_table","metadata":{"_uuid":"1d9635cb-3d25-459b-ba2d-40f3da8b8e3d","_cell_guid":"f6d73126-7eba-41ef-abd0-1b1b10039af7","trusted":true,"collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Model Selection and Performance\n\nAfter systematically evaluating various models and preprocessing scenarios, the **XGBoost** model emerged as the best performer. \n\nKey highlights include:\n\n- The XGBoost model achieved a **Mean Absolute Error (MAE) of approximately 1.47 billion IRR** and a **Root Mean Squared Error (RMSE) of approximately 4.65 billion IRR** on the test set.\n- It demonstrated a strong **R-squared score of 0.73**, indicating a good fit and explaining a significant portion of variance in housing prices.\n- The model performed best when trained with both **base features and engineered features**:\n  - Base features: `Area`, `Room`, `Parking`, `Warehouse`, `Elevator`, `AddressEncode`\n  - Engineered features: `Facilities_sum`, `Room_per_Area`, `Neighborhood_house_count`\n\n#### Why XGBoost?\n\n- XGBoost’s gradient boosting framework effectively captures complex nonlinear relationships and feature interactions.\n- It is robust to feature scaling and can handle mixed data types efficiently.\n- Inclusion of engineered features enriched the model’s ability to understand subtle patterns, improving predictive accuracy.\n\n---\n\n### Conclusion\n\nThe results underscore the value of combining feature engineering with advanced ensemble methods like XGBoost to achieve accurate and reliable housing price predictions in Tehran’s diverse real estate market.","metadata":{"_uuid":"fbe57f35-b774-458f-b2c9-7dcdd084a7b6","_cell_guid":"045ba2a8-afc9-45cf-8178-10522f0b6171","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def prepare_data (df, features, log_price=False, scale=False, log_area=False):\n    temp_df = df.copy()\n    \n    if log_price:\n        temp_df['Target'] = np.log1p(temp_df['Price'])\n    else:\n        temp_df['Target'] = temp_df['Price']\n\n    if log_area and 'Area' in features:\n        temp_df['Area'] = np.log1p(temp_df['Area'])\n\n    X = temp_df[features].copy()  \n\n    if scale:\n        scale_features = ['Area', 'Room_per_Area', 'Neighborhood_house_count', 'AddressEncode']\n        scaler = StandardScaler()\n        to_scale = [f for f in scale_features if f in X.columns]\n        X[to_scale] = scaler.fit_transform(X[to_scale])\n\n    y = temp_df['Target']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return X_train, X_test, y_train, y_test","metadata":{"_uuid":"7e2140df-a268-4fc0-80af-13b43ec84be1","_cell_guid":"cfee7e86-480c-4460-912a-c208f35f06b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features = ['Area', 'Room', 'Parking', 'Warehouse', 'Elevator',\n                     'AddressEncode', 'Facilities_sum', 'Room_per_Area', 'Neighborhood_house_count']","metadata":{"_uuid":"1753bf4a-828b-469a-bf0b-f1d8b1094890","_cell_guid":"23e3280d-eb1b-4b1c-9605-bdfe84f95180","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = prepare_data(df, selected_features, log_price, scale, log_area)\n\nbest_xgb_model = xgb.XGBRegressor()\nbest_xgb_model.fit(X_train, y_train)\n\ny_pred = best_xgb_model.predict(X_test)\n\nif log_price:\n    y_test_eval = np.expm1(y_test)\n    y_pred_eval = np.expm1(y_pred)\nelse:\n    y_test_eval = y_test\n    y_pred_eval = y_pred\n\nmae = mean_absolute_error(y_test_eval, y_pred_eval)\nrmse = np.sqrt(mean_squared_error(y_test_eval, y_pred_eval))\nr2 = r2_score(y_test_eval, y_pred_eval)\n\nprint(\"XGBoost Results\")\nprint(f\"MAE  : {mae:,.0f}\")\nprint(f\"RMSE : {rmse:,.0f}\")\nprint(f\"R²   : {r2:.3f}\")","metadata":{"_uuid":"cc67b028-761a-4221-a544-042493dc45bc","_cell_guid":"aabadbfe-d6cc-4873-bbcf-4384181b1249","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_price = False\nscale = False\nlog_area = False\n\nX_train, X_test, y_train, y_test = prepare_data(df, selected_features, log_price, scale, log_area)\n\nstacking_model = StackingRegressor(\n    estimators=[\n        ('xgb', xgb.XGBRegressor()),\n        ('tree', DecisionTreeRegressor())\n    ],\n    final_estimator=LinearRegression()\n)\n\nstacking_model.fit(X_train, y_train)\n\ny_pred = stacking_model.predict(X_test)\n\nif log_price:\n    y_test_eval = np.expm1(y_test)\n    y_pred_eval = np.expm1(y_pred)\nelse:\n    y_test_eval = y_test\n    y_pred_eval = y_pred\n\nmae = mean_absolute_error(y_test_eval, y_pred_eval)\nrmse = np.sqrt(mean_squared_error(y_test_eval, y_pred_eval))\nr2 = r2_score(y_test_eval, y_pred_eval)\n\nprint(\"Stacking Ensemble Results\")\nprint(f\"MAE  : {mae:,.0f}\")\nprint(f\"RMSE : {rmse:,.0f}\")\nprint(f\"R²   : {r2:.3f}\")","metadata":{"_uuid":"81afaff7-03ba-4e91-87c0-867414a50884","_cell_guid":"c666f2c6-0e53-40f3-aa5d-0ae9c7db912c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stacking Ensemble Model\n\nTo further improve predictive performance, a **stacking ensemble** model was developed. This model combined:\n\n- **XGBoost Regressor**: A powerful gradient boosting model.\n- **Decision Tree Regressor**: A simple tree-based model capturing different data aspects.\n\nThe final estimator used to blend predictions from base models was **Linear Regression**.\n\n#### Performance:\n\n- **Mean Absolute Error (MAE):** Approximately 1.64 billion IRR\n- **Root Mean Squared Error (RMSE):** Approximately 4.58 billion IRR\n- **R-squared (R²):** 0.738\n\n#### Interpretation:\n\n- The stacking model slightly improved the **R² score** compared to the best single XGBoost model (0.73 → 0.738), indicating better explained variance.\n- It also achieved a competitive RMSE, suggesting robust predictions with reduced error magnitude.\n- By combining strengths of multiple base models, stacking leveraged complementary insights to enhance overall accuracy.\n\n---\n\n### Summary\n\nThe stacking ensemble approach effectively aggregated different model predictions, yielding improved accuracy and reliability for Tehran housing price prediction. This demonstrates the power of ensemble learning in complex regression tasks.","metadata":{"_uuid":"73b473cc-8a18-4bb3-9cd3-e86fb23e1b23","_cell_guid":"845e5a3d-bf35-4532-a90c-853404396c17","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Comparison of Top Machine Learning Models\n\n| Model           | MAE (IRR)          | RMSE (IRR)         | R² Score |\n|-----------------|--------------------|--------------------|----------|\n| XGBoost         | 1,472,786,064      | 4,652,751,348      | 0.730    |\n| Stacking (XGB + Decision Tree) | 1,638,821,150      | 4,578,545,931      | 0.738    |\n\n#### Analysis:\n\n- The **XGBoost model** achieved the lowest MAE, indicating slightly better average error on price predictions.\n- The **stacking ensemble** improved the R² score marginally, showing a better overall fit and ability to explain variance in the data.\n- Stacking’s RMSE was slightly lower than XGBoost’s, suggesting it reduced larger prediction errors somewhat, despite a higher MAE.\n- These results suggest that while XGBoost alone is a strong and competitive model, combining it with another model in a stacking ensemble can yield modest improvements in explained variance and error distribution.\n\n---\n\n### Summary for Machine Learning Models\n\nThe experiments demonstrated that tree-based gradient boosting models like XGBoost are highly effective for housing price prediction due to their robustness and ability to capture nonlinear relationships.\n\nFurthermore, ensemble techniques such as stacking can leverage complementary strengths of different models, resulting in more reliable and slightly more accurate predictions.\n\nHowever, the improvements from stacking over a well-tuned XGBoost model were modest, indicating that XGBoost alone already captures most of the meaningful patterns in the dataset.\n\nOverall, tree-based ensembles with thoughtful feature engineering provide a powerful baseline for this regression task and serve as a strong foundation for further enhancements or integration with deep learning methods.","metadata":{"_uuid":"0abcc709-1bcb-4f7a-9a4d-94787e898298","_cell_guid":"6b36e34a-d2a2-4196-b5d8-4ab045548384","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### 6.2 Deep Learning Model\n\nFor comparison with traditional machine learning models, a fully connected Artificial Neural Network (ANN) was implemented with the following characteristics:\n\n- **Architecture:**\n  - Three hidden layers with 256, 128, and 64 neurons respectively.\n  - ReLU activation functions and Batch Normalization layers to improve training stability and convergence.\n- **Training setup:**\n  - Optimizer: Adam\n  - Loss function: Huber loss, which is robust to outliers.\n  - Early stopping and learning rate reduction callbacks to avoid overfitting and optimize training.\n  - Log-transformation and scaling applied to input features and target variable.\n\n#### Performance:\n\n- **Mean Absolute Error (MAE):** Approximately 1.89 billion IRR\n- **Root Mean Squared Error (RMSE):** Approximately 4.69 billion IRR\n- **R-squared (R²):** 0.725\n\n#### Interpretation:\n\n- The ANN model achieved competitive results, closely matching the performance of tree-based models.\n- Its R² score of 0.725 indicates it explains a substantial portion of variance in housing prices.\n- Slightly higher MAE compared to XGBoost suggests that while ANN captures complex nonlinearities, tree-based gradient boosting models remain more effective on this dataset.\n- The careful use of log-transformations, batch normalization, and early stopping helped stabilize training and improved generalization.\n\n---\n\n### Summary\n\nThe deep learning approach demonstrated strong predictive ability, validating its feasibility for housing price regression tasks.\n\nHowever, given the slightly better metrics from ensemble tree-based methods, further tuning or more advanced architectures might be needed to surpass them.\n\nThis comparison highlights that for tabular data with engineered features, gradient boosting models often provide robust baselines, while deep learning can complement and potentially improve upon them with additional optimization.","metadata":{"_uuid":"e93c9ac5-3ec4-4b08-a89a-7da90b86616a","_cell_guid":"4f69d68a-35cc-4233-8230-cc02ab7bc08d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = prepare_data(\n    df, engineered_features + baseFeatures,\n    log_price=True, scale=True, log_area=True\n)","metadata":{"_uuid":"5b8f63d8-5b26-4a61-b72b-daf7cc2ec54c","_cell_guid":"349bee44-1b11-4be1-b82a-6ecde00e833f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    layers.BatchNormalization(),\n    layers.Dense(128, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\n\nmodel.compile(optimizer='adam', loss=keras.losses.Huber(delta=1.0))\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.1,\n    epochs=200,\n    batch_size=64,\n    verbose=1,\n    callbacks=[\n        keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True),\n        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n    ]\n)","metadata":{"_uuid":"6f873161-810a-4e7b-bcd6-3a4d2bac56ce","_cell_guid":"f1a3eb55-bc75-4dc3-aa66-6a02d977df0b","trusted":true,"collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = model.predict(X_test).flatten()\n\npreds_price = np.expm1(preds)\ny_test_price = np.expm1(y_test)\n\nmae = mean_absolute_error(y_test_price, preds_price)\nrmse = np.sqrt(mean_squared_error(y_test_price, preds_price))\nr2 = r2_score(y_test_price, preds_price)\n\nprint(\"ANN Results\")\nprint(f\"MAE  : {mae:,.0f}\")\nprint(f\"RMSE : {rmse:,.0f}\")\nprint(f\"R²   : {r2:.3f}\")","metadata":{"_uuid":"d0153d6b-c695-4135-b73e-464ca2edcc0e","_cell_guid":"5280f10c-1672-41d7-8887-c6c09814865d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7 - Project Summary and Key Findings\n\nThis project aimed to develop an accurate predictive model for housing prices in Tehran, leveraging a diverse set of features and advanced machine learning techniques.\n\n### Workflow Overview:\n\n- **Data Exploration and Cleaning:** Initial data understanding, handling missing values, and outlier detection were performed to ensure data quality.\n- **Feature Engineering:** Created meaningful features such as the sum of facilities, room density (`Room_per_Area`), and neighborhood house counts to enrich the dataset.\n- **Preprocessing:** Applied appropriate transformations including encoding categorical variables, optional log-transformations, and scaling depending on model requirements.\n- **Model Training and Evaluation:** Tested a wide spectrum of models ranging from linear regression, tree-based ensemble methods, to deep learning models.\n- **Ensemble Learning:** Used stacking to combine complementary models, achieving slight improvements over single models.\n\n### Key Results:\n\n- The **XGBoost** model with engineered features delivered strong performance, achieving MAE around **1.47 billion IRR** and R² of **0.73**.\n- The **Stacking ensemble** (XGBoost + Decision Tree) slightly improved the R² to **0.738**, showcasing the power of combining models.\n- The **Artificial Neural Network (ANN)** demonstrated competitive results with MAE around **1.89 billion IRR** and R² of **0.725**.\n- Feature engineering and careful preprocessing were critical in boosting model accuracy.\n- Overall, tree-based gradient boosting models remain the most effective for this tabular regression task, though deep learning shows promising potential.\n\n---\n\n### Top Model Performance Comparison\n\n| Model             | MAE (IRR)          | RMSE (IRR)         | R² Score | MAE (USD)   | RMSE (USD)  |\n|-------------------|--------------------|--------------------|----------|-------------|-------------|\n| XGBoost           | 1,472,786,064      | 4,652,751,348      | 0.730    | 16,256  | 51,355  |\n| Stacking Ensemble  | 1,638,821,150      | 4,578,545,931      | 0.738    | 18,089  | 50,536 |\n| ANN (Deep Learning)| 1,886,608,949      | 4,693,735,608      | 0.725    | 20,823  | 51,807  |\n\n*Note: USD values calculated assuming an exchange rate of 90,000 IRR per USD.*\n\n---\n\n### Final Remarks\n\nThis project illustrates the value of combining thorough exploratory data analysis, feature engineering, and advanced modeling techniques to predict real estate prices effectively. The results can serve as a solid foundation for practical pricing tools or further research integrating additional spatial and temporal data.","metadata":{"_uuid":"8d6eb188-b8d1-4638-9d14-76c5e410f9b4","_cell_guid":"8bd2e4f2-0c58-49db-ba39-c5107cbf98df","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}